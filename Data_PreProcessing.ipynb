{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import stumpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Raw Data Files and Joining Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Sensors Data that has Location and sensortype_id\n",
    "\n",
    "sensor_df = pd.read_csv('./Data/Sensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading SensorType Data that has sensortype_id, sensor name and sensor type\n",
    "\n",
    "sensor_type_df = pd.read_csv('./Data/SensorType.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only Relevant Columns from Sensor Data (sensor_df)\n",
    "\n",
    "sensor_df = sensor_df[['id', 'sensortype_id', 'location_identifier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Relevant Columns from Sensor Data (sensor_df)\n",
    "\n",
    "sensor_df.columns = ['sensor_id', 'sensortype_id', 'location_identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining Sensor Data (sensor_df) and SensorType Data (sensor_type_df) based on the sensortype_id\n",
    "\n",
    "sensor_data = sensor_df.merge(sensor_type_df, left_on='sensortype_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Sensor Data after the join\n",
    "\n",
    "sensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only Relevant Columns from Joined Sensor Data and SensorType Data (sensor_data)\n",
    "\n",
    "sensor_data = sensor_data[['sensor_id', 'location_identifier', 'name', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading SensorHistory Data (sensor_history_df) that has sensor_id, Timestamp, sensor_reading and id\n",
    "\n",
    "sensor_history_df = pd.read_csv(\"./Data/SensorDataHistorylarge.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Relevant Columns from SensorHinstory Data (sensor_history_df)\n",
    "\n",
    "sensor_history_df.columns = [\"reading_id\", \"sensor_id\", \"value\", \"value1\", \"value2\", \"Timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 2 Columns (value1 and value2) from SensorHinstory Data (sensor_history_df) \n",
    "# as all the values in these columns were missing (NaN)\n",
    "\n",
    "sensor_history_df.drop([\"value1\", \"value2\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining Previously Joined Sensor Data and SensorType Data (sensor_data) with our \n",
    "# SensorHistory Data (sensor_history_df) based on the sensor_id\n",
    "\n",
    "sensor_data = sensor_history_df.merge(sensor_data, on='sensor_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Timestamp Column of Joined Data (sensor_data) from string type to datetime type \n",
    "\n",
    "sensor_data['Timestamp'] = sensor_data['Timestamp'].astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the sensor_data by Timestamp column\n",
    "\n",
    "sensor_data = sensor_data.sort_values(by='Timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Time Series DataFrame of Sensor Readings From Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Timestamp Column of Joined Data (sensor_data) from string type to datetime type \n",
    "\n",
    "sensor_data['Timestamp'] = sensor_data['Timestamp'].astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping Over all the sensor_ids in the sensor_data and selecting the Timestamp and value(sensor_reading) column\n",
    "# This creates a dictionary (dfs) where key is the sensor_id and value is a dataframe containing rows \n",
    "# that have only this specific sensor_id \n",
    "\n",
    "dfs = {}\n",
    "for sensor_id in tqdm(sensor_data.sensor_id.unique()):\n",
    "    dfs[sensor_id] = sensor_data[sensor_data[\"sensor_id\"] == sensor_id][[\"Timestamp\", \"value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping Over all the key, value pairs in the dictionary(dfs) and renaming the dataframe on the value of this dict  \n",
    "for key, value in tqdm(dfs.items()):\n",
    "    value.columns = [\"Timestamp\", key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Dictionary of Dataframes to a list of dfs\n",
    "\n",
    "dfs = [value for value in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping over all the list items in the dataframe and performing downsampling \n",
    "#  interval is 5 minutes \n",
    "\n",
    "new_dfs = []\n",
    "for df in tqdm(dfs):\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "    df = df.resample('5T', on=\"Timestamp\").mean()\n",
    "    new_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining First 2 values in the list(new_dfs) on Timestamp\n",
    "\n",
    "outer_join_df = pd.merge(new_dfs[0], new_dfs[1], on=\"Timestamp\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining rest of the values from 2 to onwards in the list(new_dfs) on Timestamp\n",
    "\n",
    "for value in tqdm(new_dfs[2:]):\n",
    "    outer_join_df = pd.merge(outer_join_df, value, on='Timestamp', how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the final result of join performed above\n",
    "outer_join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame from the time series DataFrame (outer_join_df)\n",
    "\n",
    "summary = outer_join_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Summary DataFrame created from outer_join_df \n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of Sensor_ids where total number of Non-null values are less than 50\n",
    "invaluable_cols = [summary_col for summary_col in summary.columns if summary[summary_col]['count'] < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the count of total useless sensor_ids\n",
    "len(invaluable_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all rows from sensor_data where these useless sensor_ids exist\n",
    "\n",
    "\n",
    "for column in tqdm(invaluable_cols):\n",
    "    condition = (sensor_data['sensor_id'] == column)\n",
    "    sensor_data = sensor_data[~condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the useless columns/sensor_ids from the Time Series DataFrame\n",
    "outer_join_df.drop(invaluable_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving sensor_data as a csv file in the Processed_Data Directory\n",
    "\n",
    "sensor_data.to_csv('./Processed_Data/Sensors_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Time Series DataFrame as a csv file in Processed Data Directory\n",
    "\n",
    "outer_join_df.to_csv(\"./Processed_Data/Final_Sensor_Time_Series.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Processed Data File and Create Different Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading sensor_data from csv file Sensor_Data.csv in the Processed_Data Directory\n",
    "\n",
    "sensor_data = pd.read_csv('./Processed_Data/Sensors_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying sensor_data read from csv file Sensor_Data.csv in the Processed_Data Directory\n",
    "\n",
    "sensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Representation of SensorType Counts in Location\n",
    "# Grouping Data First on SensorTyp and then on Location\n",
    "\n",
    "sensor_location_type_count = sensor_data.groupby(['name', \"location_identifier\"])['sensor_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstacking the multiple level of grouping\n",
    "\n",
    "sensor_location_type_count = sensor_location_type_count.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the NaN values with 0\n",
    "sensor_location_type_count.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the Representation\n",
    "sensor_location_type_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the Representation of Sensortype Counts\n",
    "sensor_location_type_count.to_csv('./Processed_Data/Sensor_Location_Type_Count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_type_names_dict = {}\n",
    "sensor_type_names = sensor_data.groupby(['type'])['name'].unique()\n",
    "\n",
    "for key in sensor_type_names.keys():\n",
    "    sensor_type_names_dict[key] = sensor_type_names[key].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Dictionary of sensor names\n",
    "with open('./Processed_Data/sensor_type_names_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(sensor_type_names_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Representation of Sensor Ids for each Sensor Type in Location\n",
    "# Grouping Data First on SensorType(name) and then on Location(location_identifier)\n",
    "\n",
    "sensor_location_type_ids = sensor_data.groupby(['name', \"location_identifier\"])['sensor_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstacking the multiple level of grouping\n",
    "\n",
    "sensor_location_type_ids = sensor_location_type_ids.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an additional rows in the data based on the sensor types 'bool_list' containing bool type sensor_ids and 'decimal_list' containing decimal type sensor_ids \n",
    "\n",
    "for column in sensor_location_type_ids.columns:\n",
    "    for key, values in sensor_type_names_dict.items():\n",
    "        sensor_location_type_ids.loc[f'{key}_list', column] = [f\"{item}\" for sublist in sensor_location_type_ids.loc[values, column] if isinstance(sublist, np.ndarray) for item in sublist]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the Representation\n",
    "\n",
    "sensor_location_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Representation \n",
    "sensor_location_type_ids.to_csv('./Processed_Data/Sensor_Location_Type_Ids.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Interpolation to Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Time Series DataFrame from Processed Data Directory\n",
    "final_data = pd.read_csv(\"./Processed_Data/Final_Sensor_Time_Series.csv\", index_col='Timestamp', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Representation of Sensor Ids for each Sensor Type in Location from Processed_Data Directory\n",
    "sensor_location_type_ids = pd.read_csv('./Processed_Data/Sensor_Location_Type_Ids.csv', index_col='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Dictionary of sensor names for a specific Sensor Type to pickle file in Processed_Data Directory\n",
    "with open('./Processed_Data/sensor_type_names_dict.pkl', 'rb') as f:\n",
    "    sensor_type_names_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the boolean column/sensor_ids to boolean from sensor_location_type_ids \n",
    "\n",
    "bool_columns = eval(sensor_location_type_ids.loc[f'bool_list', 'B-1 407'])\n",
    "\n",
    "for column in bool_columns:\n",
    "    conditions = [(final_data[column] == 0.), (~final_data[column].isna())]\n",
    "    values = [False, True]\n",
    "    final_data[column] = (np.select(conditions, values, default=np.NaN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of sensor_ids/columns from sensor_location_type_\n",
    "columns = []\n",
    "for key in sensor_type_names_dict.keys():\n",
    "    columns.extend(eval(sensor_location_type_ids.loc[f'{key}_list', 'B-1 407']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Mapping of Sensor id to Device Type\n",
    "\n",
    "sensors_id_type = sensor_location_type_ids.loc[:,'B-1 407'].to_dict()\n",
    "\n",
    "sensor_id_type_mapping = {}\n",
    "for key, values in sensors_id_type.items():\n",
    "    if isinstance(values, str)  and 'list' not in key:\n",
    "        for value in eval(values):\n",
    "            sensor_id_type_mapping[f\"{value}\"] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Selected Columns as Time Series\n",
    "\n",
    "plt.rc('figure', figsize=(15, 8))\n",
    "for column in columns:\n",
    "    final_data[column].plot(label=sensor_id_type_mapping[column])\n",
    "    plt.ylabel(column, fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using STL for Seasonal decomposition and Interpolation data with \n",
    "#using 2 different interpolation methods 1 is with seasonality \n",
    "# The 2nd one is a Linear interpolation \n",
    "# Also applying forward filling and backward filling for booleans\n",
    "\n",
    "for column in bool_columns:\n",
    "    final_data[column] = final_data[column].fillna(method='ffill')\n",
    "    final_data[column] = final_data[column].fillna(method='bfill')\n",
    "    \n",
    "for column in columns:\n",
    "    if final_data[column].isna().any():\n",
    "        res = STL(final_data[column].interpolate(method='linear').fillna(method='bfill'), seasonal=15, period=12*24).fit()\n",
    "        seasonal_component = res.seasonal\n",
    "\n",
    "        if not seasonal_component.isna().all():\n",
    "            deseasonalized_data = final_data[column] - seasonal_component\n",
    "            deseasonalized_data_imputed = deseasonalized_data.interpolate(method='linear')\n",
    "            values = (deseasonalized_data_imputed + seasonal_component).fillna(method='bfill')\n",
    "            final_data[column] = values\n",
    "\n",
    "        else:\n",
    "            deseasonalized_data_imputed = final_data[column].interpolate(method='linear')\n",
    "            final_data[column] = deseasonalized_data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Selected Columns \n",
    "plt.rc('figure', figsize=(15, 8))\n",
    "for column in columns:\n",
    "    final_data[column].plot(label=sensor_id_type_mapping[column])\n",
    "    plt.ylabel(column, fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of all the boolean columns/sensor_ids \n",
    "all_bool_columns = [f\"{item}\" for sublist in sensor_location_type_ids.loc['bool_list', :] \n",
    "                                                       if isinstance(eval(sublist), list) \n",
    "                                                       for item in eval(sublist)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the boolean column/sensor_ids to boolean\n",
    "# \n",
    "# If a value is 0 it is set as False if value is greater than 0 it will be set as True and NaN will be NaN\n",
    "\n",
    "for column in tqdm(all_bool_columns):\n",
    "    conditions = [(final_data[column] == 0.), (~final_data[column].isna())]\n",
    "    values = [False, True]\n",
    "    final_data[column] = (np.select(conditions, values, default=np.NaN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying forward filling and backward filling for all the boolea\n",
    "for column in tqdm(all_bool_columns):\n",
    "    final_data[column] = final_data[column].fillna(method='ffill')\n",
    "    final_data[column] = final_data[column].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using STL for Seasonal decomposition and Interpolation data with \n",
    "#using 2 different interpolation methods 1 is with seasonality \n",
    "# The 2nd one is a Linear interpolation \n",
    "# Also applying forward filling and backward filling for booleans\n",
    "columns = final_data.columns\n",
    "columns = list(set(columns) - set(all_bool_columns))\n",
    "\n",
    "for column in tqdm(columns):\n",
    "    res = STL(final_data[column].interpolate(method='linear').fillna(method='bfill'), seasonal=15, period=12*24).fit()\n",
    "    seasonal_component = res.seasonal\n",
    "    \n",
    "    if not seasonal_component.isna().all():\n",
    "        deseasonalized_data = final_data[column] - seasonal_component\n",
    "        deseasonalized_data_imputed = deseasonalized_data.interpolate(method='linear')\n",
    "        values = (deseasonalized_data_imputed + seasonal_component).fillna(method='bfill')\n",
    "        final_data[column] = values\n",
    "    \n",
    "    else:\n",
    "        deseasonalized_data_imputed = final_data[column].interpolate(method='linear')\n",
    "        final_data[column] = deseasonalized_data_imputed.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if decimal Columns have any NaN values left\n",
    "\n",
    "for column in tqdm(columns):\n",
    "    if final_data[column].isna().any():\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if Time for the Minimum DateTime is 00:00AM if not then select data from the subsequent date\n",
    "\n",
    "first_date = final_data.index.min()\n",
    "\n",
    "if not first_date.time() == pd.to_datetime('00:00:00').time():\n",
    "    final_data = final_data[final_data.index.date > first_date.date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Time Series DataFrame as a csv file in Processed Data Directory\n",
    "\n",
    "final_data.to_csv(\"./Processed_Data/Final_Sensor_Time_Series_Imputed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Multidimensional Matrix Profile (MMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Representation of Sensor Ids for each Sensor Type in Location from Processed_Data Directory\n",
    "\n",
    "sensor_location_type_ids = pd.read_csv('./Processed_Data/Sensor_Location_Type_Ids.csv', index_col='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Dictionary of sensor names for a specific Sensor Type to pickle file in Processed_Data Directory\n",
    "\n",
    "with open('./Processed_Data/sensor_type_names_dict.pkl', 'rb') as f:\n",
    "    sensor_type_names_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_input = input(\"Enter a Specific Location For MMP: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of sensor_ids/columns from sensor_location_type_ids\n",
    "\n",
    "columns = []\n",
    "for key in sensor_type_names_dict.keys():\n",
    "    columns.extend(eval(sensor_location_type_ids.loc[f'{key}_list', location_input]))\n",
    "    \n",
    "columns.append('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Time Series DataFrame from Processed Data Directory for user selected Location only\n",
    "\n",
    "final_data = pd.read_csv(\"./Processed_Data/Final_Sensor_Time_Series_Imputed.csv\", usecols=columns, \n",
    "                         index_col='Timestamp', parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Time Series DataFrame for selexted Location\n",
    "\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Mapping of Sensor id to Device Type for a Specific User Selected Location \n",
    "\n",
    "sensors_id_type = sensor_location_type_ids.loc[:,location_input].to_dict()\n",
    "\n",
    "sensor_id_type_mapping = {}\n",
    "for key, values in sensors_id_type.items():\n",
    "    if isinstance(values, str)  and 'list' not in key:\n",
    "        for value in eval(values):\n",
    "            sensor_id_type_mapping[f\"{value}\"] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_id_type_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Multidimensional Matrix Profile\n",
    "m = 2016\n",
    "\n",
    "mps, indices = stumpy.mstump_m(final_data, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the shape of MultiDimensional Matrix Profile \n",
    "\n",
    "mps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the motifs_idx based on the standard np.argmin method \n",
    "motifs_idx = np.argmin(mps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the motifs of each dimension\n",
    "\n",
    "motifs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the nearest neighbours of each individual motifs_idx\n",
    "\n",
    "nn_idx = indices[np.arange(len(motifs_idx)), motifs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the date index \n",
    "df = final_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the dimensions of the time series dataframe df\n",
    "\n",
    "fig, axs = plt.subplots(mps.shape[0] * 2, sharex=True, gridspec_kw={'hspace': 0}, figsize=(15, 20))\n",
    "\n",
    "for k, dim_name in enumerate(df.columns):\n",
    "    axs[k].set_ylabel(dim_name, fontsize=10)\n",
    "    axs[k].set_xlabel('Time', fontsize=10) \n",
    "    axs[k].plot(df[dim_name], label=sensor_id_type_mapping[dim_name])\n",
    "    axs[k].legend(loc=\"upper right\")\n",
    "    axs[k].plot(range(motifs_idx[k], motifs_idx[k] + m), df[dim_name].iloc[motifs_idx[k] : motifs_idx[k] + m], c='red', linewidth=4)\n",
    "    axs[k].plot(range(nn_idx[k], nn_idx[k] + m), df[dim_name].iloc[nn_idx[k] : nn_idx[k] + m], c='red', linewidth=4)\n",
    "    \n",
    "    axs[k].axvline(x=motifs_idx[k], linestyle=\"dashed\", c='black')\n",
    "    axs[k].axvline(x=nn_idx[k], linestyle=\"dashed\", c='black')\n",
    "\n",
    "    axs[k + mps.shape[0]].set_ylabel(f\"P_{dim_name}\", fontsize=10)\n",
    "    axs[k + mps.shape[0]].plot(mps[k], c='orange')\n",
    "    axs[k + mps.shape[0]].set_xlabel('Time', fontsize=10)    \n",
    "    \n",
    "    axs[k + mps.shape[0]].axvline(x=motifs_idx[k], linestyle=\"dashed\", c='black')\n",
    "    axs[k + mps.shape[0]].axvline(x=nn_idx[k], linestyle=\"dashed\", c='black')    \n",
    "    \n",
    "    axs[k + mps.shape[0]].plot(motifs_idx[k], mps[k, motifs_idx[k]] + 1, marker=\"v\", markersize=10, color='red')\n",
    "    axs[k + mps.shape[0]].plot(nn_idx[k], mps[k, nn_idx[k]] + 1, marker=\"v\", markersize=10, color='red')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting motifs_idx based on a threshold. \n",
    "threshold = int(input(\"Enter a Specific Threshold Value For Motif Selection: \"))\n",
    "motifs_idx = np.argwhere(mps < threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of motifs for each dimension selected using a threshold \n",
    "\n",
    "motifs = {}\n",
    "for i in range(0, len(motifs_idx)):\n",
    "    if motifs.get(motifs_idx[i][0]):\n",
    "        motifs[motifs_idx[i][0]].append(motifs_idx[i][1])\n",
    "    else:\n",
    "        motifs.setdefault(motifs_idx[i][0], []).append(motifs_idx[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Processing\n",
    "\n",
    "update_motifs = {}\n",
    "for key, motifs_idx in motifs.items():\n",
    "    for i in range(0, len(motifs_idx), 2016):\n",
    "        if update_motifs.get(key):\n",
    "            update_motifs[key].append(motifs_idx[i])\n",
    "        else:\n",
    "            update_motifs.setdefault(key, []).append(motifs_idx[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Nearest Neighbour that correspond to each motif_idx in the update_motifs dict\n",
    "\n",
    "final_nn = {}\n",
    "final_motifs = {}\n",
    "\n",
    "for key, values in update_motifs.items():\n",
    "    nns = {}\n",
    "    for value in values:\n",
    "        if nns.get(value) is None:\n",
    "            if final_nn.get(key):\n",
    "                final_nn[key].append(indices[key, value])\n",
    "                final_motifs[key].append(value)\n",
    "            else:\n",
    "                final_nn.setdefault(key, []).append(indices[key, value])\n",
    "                final_motifs.setdefault(key, []).append(value)\n",
    "        \n",
    "            nns[indices[key, value]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the the final_motifs dict\n",
    "# Key is the dimension index of mps like 0, 1, 2\n",
    "\n",
    "final_motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Nearest Neighbour of Each Motif Selected based on user defined threshold \n",
    "final_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the dimensions of the time series dataframe df \n",
    "fig, axs = plt.subplots(mps.shape[0] * 2, sharex=True, gridspec_kw={'hspace': 0}, figsize=(15, 20))\n",
    "\n",
    "for k, dim_name in enumerate(df.columns):\n",
    "    axs[k].set_ylabel(dim_name, fontsize=10)\n",
    "    axs[k].set_xlabel('Time', fontsize=10) \n",
    "    axs[k].plot(df[dim_name], label=sensor_id_type_mapping[dim_name])\n",
    "    axs[k].legend(loc=\"upper right\")\n",
    "    i = 0\n",
    "    if final_motifs.get(k) and final_nn.get(k):\n",
    "        for motifs_idx, nn_idx in zip(final_motifs.get(k), final_nn.get(k)):\n",
    "            \n",
    "            axs[k].plot(df[dim_name].iloc[motifs_idx : motifs_idx + m], c='red', linewidth=4)\n",
    "            axs[k].plot(df[dim_name].iloc[nn_idx : nn_idx + m], c='red', linewidth=4)\n",
    "            axs[k].axvline(x=motifs_idx, linestyle=\"dashed\", c='black')\n",
    "            axs[k].axvline(x=nn_idx, linestyle=\"dashed\", c='black')\n",
    "            \n",
    "            axs[k + mps.shape[0]].plot(motifs_idx, mps[k, motifs_idx] + 1, marker=\"v\", markersize=10, color='red')\n",
    "            axs[k + mps.shape[0]].plot(nn_idx, mps[k, nn_idx] + 1, marker=\"v\", markersize=10, color='red')\n",
    "\n",
    "            axs[k + mps.shape[0]].axvline(x=motifs_idx, linestyle=\"dashed\", c='black')\n",
    "            axs[k + mps.shape[0]].axvline(x=nn_idx, linestyle=\"dashed\", c='black')\n",
    "            \n",
    "            axs[k + mps.shape[0]].text(motifs_idx, mps[k][motifs_idx], f\"{i+1}m\", fontsize=\"xx-large\")\n",
    "            axs[k + mps.shape[0]].text(nn_idx, mps[k][motifs_idx], f\"{i+1}n\", fontsize=\"xx-large\")\n",
    "            i += 1\n",
    "    axs[k + mps.shape[0]].set_ylabel(f\"P_{dim_name}\", fontsize=10)\n",
    "    axs[k + mps.shape[0]].plot(mps[k], c='orange')\n",
    "    axs[k + mps.shape[0]].set_xlabel('Time', fontsize=10)    \n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold/Exclusion Zone For Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Representation of Sensor Ids for each Sensor Type in Location from Processed_Data\n",
    "sensor_location_type_ids = pd.read_csv('./Processed_Data/Sensor_Location_Type_Ids.csv', index_col='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Dictionary of sensor names for a specific Sensor Type to pickle file in Processed_Data \n",
    "with open('./Processed_Data/sensor_type_names_dict.pkl', 'rb') as f:\n",
    "    sensor_type_names_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_input = input(\"Enter a Specific Location For MMP: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for key in sensor_type_names_dict.keys():\n",
    "    columns.extend(eval(sensor_location_type_ids.loc[f'{key}_list', location_input]))\n",
    "    \n",
    "columns.append('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Time Series DataFrame from Processed Data Directory for user selected Location only\n",
    "\n",
    "final_data = pd.read_csv(\"./Processed_Data/Final_Sensor_Time_Series_Imputed.csv\", usecols=columns, \n",
    "                         index_col='Timestamp', parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Multidimensional Matrix Profile\n",
    "m = 2016\n",
    "\n",
    "mps, indices = stumpy.mstump_m(final_data, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the motifs_idx based on the standard np.argmin\n",
    "\n",
    "\n",
    "motifs_idx = np.argmin(mps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the nearest neighbours\n",
    "\n",
    "nn_idx = indices[np.arange(len(motifs_idx)), motifs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls, subspaces = stumpy.mdl(final_data, m, motifs_idx, nn_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(mdls)), mdls, c='red', linewidth='4')\n",
    "plt.xlabel('k (zero-based)', fontsize='20')\n",
    "plt.ylabel('Bit Size', fontsize='20')\n",
    "plt.xticks(range(mps.shape[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = final_data.columns[subspaces[k]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Multidimensional Matrix Profile For a weekly rhythm where subsequent length m=2016\n",
    "m = 2016\n",
    "\n",
    "mps, indices = stumpy.mstump_m(final_data, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Processing the motifs \n",
    "\n",
    "def get_percent_motif_start_index(motifs):\n",
    "    final_motifs = []\n",
    "    \n",
    "    for i in range(0, len(motifs), 2016):\n",
    "        final_motifs.append(motifs.index[i])\n",
    "            \n",
    "    return final_motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Processing the motifs\n",
    "\n",
    "def get_motif_start_index(motifs):\n",
    "    final_motifs = []\n",
    "    j = 0\n",
    "    check = False\n",
    "    \n",
    "    for i in range(0, len(motifs), 2016):\n",
    "        if not check:\n",
    "            j = i\n",
    "\n",
    "        final_motifs.append(motifs[j])\n",
    "\n",
    "        if check:\n",
    "            j = i + value\n",
    "\n",
    "        elif motifs[i] + 2016 > len(motifs):\n",
    "            value = len(motifs) - motifs[i]\n",
    "            j = i + value\n",
    "            check = True\n",
    "            \n",
    "                \n",
    "    return final_motifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that would take in percentage for discord and percentage for motif \n",
    "\n",
    "def select_motifs_discords_percentage(mps, dimension, motif_percentage, discord_percentage, motifs={}, discords={}):\n",
    "    motif_threshold = mps.quantile(motif_percentage/100)\n",
    "    discord_threshold = mps.quantile((100 - discord_percentage)/100)\n",
    "    \n",
    "    motif = mps[mps < motif_threshold]\n",
    "    discord = mps[mps > discord_threshold]\n",
    "    \n",
    "    if len(motif):\n",
    "        \n",
    "        motifs[dimension] = get_percent_motif_start_index(motif)\n",
    "        \n",
    "    return motifs, discords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Upper K of all points and Lowest J of all points as discords and motifs from mps\n",
    "\n",
    "def select_top_k_motifs_discords(mps, dimension, k_motifs, k_discords, motifs={}, discords={}):\n",
    "    # Getting the motifs_idx based on the standard np.argmin method \n",
    "    # It gives 1 single motif for each dimension\n",
    "    sorted_mps = np.argsort(mps, kind='stable')\n",
    "    \n",
    "    motifs[dimension] = get_motif_start_index(sorted_mps)[:k_motifs]\n",
    "    \n",
    "    return motifs, discords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking user's input for function to be applied on each dimension and Parameters of those functions\n",
    "motif_thresholds_for_all_dimensions = {}\n",
    "\n",
    "for dimension in range(mps.shape[0]):\n",
    "    motif_thresholds_for_single_dimension = {}\n",
    "    function = input(f\"Select A Function To Be Applied to Dimension {dimension}\\n\"\n",
    "                     f\"\\tPress 1 For Selection Based on Percentage\\n \"\n",
    "                     f\"\\tPress 2 For Top K Motifs and Discords Selection: \")\n",
    "    \n",
    "    if int(function) == 1:\n",
    "        motif_percentage = input(f\"\\tEnter a Specific Threshold Value For Motif Selection For Dimension {dimension}: \")\n",
    "        discord_percentage = input(f\"\\tEnter a Specific Threshold Value For Discord Selection For Dimension {dimension}: \")\n",
    "        motif_thresholds_for_single_dimension[\"function\"] = int(function)\n",
    "        motif_thresholds_for_single_dimension[\"motif_percentage\"] = int(motif_percentage)\n",
    "        motif_thresholds_for_single_dimension[\"discord_percentage\"] = int(discord_percentage)\n",
    "    \n",
    "    elif int(function) == 2:\n",
    "        k_motif = input(f\"\\tEnter Top K Motif Selection For Dimension {dimension}: \")\n",
    "        k_discord = input(f\"\\tEnter Top K Discord Selection For Dimension {dimension}: \")\n",
    "        motif_thresholds_for_single_dimension[\"function\"] = int(function)\n",
    "        motif_thresholds_for_single_dimension[\"k_motifs\"] = int(k_motif)\n",
    "        motif_thresholds_for_single_dimension[\"k_discords\"] = int(k_discord)\n",
    "    else:\n",
    "        continue\n",
    "    motif_thresholds_for_all_dimensions[dimension] = motif_thresholds_for_single_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Dictionary of Each Dimensions Function and Function Parameters \n",
    "\n",
    "motif_thresholds_for_all_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Motifs and Discords \n",
    "motifs = {}\n",
    "discords = {}\n",
    "\n",
    "mps_df = pd.DataFrame(mps).T\n",
    "\n",
    "for key, value in motif_thresholds_for_all_dimensions.items():\n",
    "    if value.get('function') == 1:\n",
    "        motifs, discords = select_motifs_discords_percentage(mps_df[key], key, value[\"motif_percentage\"], \n",
    "                                                             value[\"discord_percentage\"], motifs, discords)\n",
    "    elif value.get('function') == 2:\n",
    "        motifs, discords = select_top_k_motifs_discords(mps_df[key], key, value['k_motifs'], \n",
    "                                                        value['k_discords'], motifs, discords)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Each Motif Selected based on user defined Parameters\n",
    "motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Nearest Neighbour that correspond to each motif_idx in the final_motifs dict\n",
    "\n",
    "final_nn = {}\n",
    "final_motifs = {}\n",
    "\n",
    "for key, values in motifs.items():\n",
    "    nns = {}\n",
    "    for value in values:\n",
    "        if nns.get(value) is None:\n",
    "            if final_nn.get(key):\n",
    "                final_nn[key].append(indices[key, value])\n",
    "                final_motifs[key].append(value)\n",
    "            else:\n",
    "                final_nn.setdefault(key, []).append(indices[key, value])\n",
    "                final_motifs.setdefault(key, []).append(value)\n",
    "        \n",
    "            nns[indices[key, value]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Each Motif Selected based on user defined Paramters For Each Dimension After filtering\n",
    "\n",
    "final_motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Nearest Neighbour of Each Motif Selected based on user defined Parameters For Each Dimension\n",
    "\n",
    "final_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_id_type = sensor_location_type_ids.loc[:,location_input].to_dict()\n",
    "\n",
    "sensor_id_type_mapping = {}\n",
    "for key, values in sensors_id_type.items():\n",
    "    if isinstance(values, str)  and 'list' not in key:\n",
    "        for value in eval(values):\n",
    "            sensor_id_type_mapping[f\"{value}\"] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the dimensions of the time series dataframe df \n",
    "fig, axs = plt.subplots(mps.shape[0] * 2, sharex=True, gridspec_kw={'hspace': 0}, figsize=(15, 20))\n",
    "\n",
    "for k, dim_name in enumerate(df.columns):\n",
    "    axs[k].set_ylabel(dim_name, fontsize=10)\n",
    "    axs[k].set_xlabel('Time', fontsize=10) \n",
    "    axs[k].plot(df[dim_name], label=sensor_id_type_mapping[dim_name])\n",
    "    axs[k].legend(loc=\"upper right\")\n",
    "    i = 0\n",
    "    if final_motifs.get(k) and final_nn.get(k):\n",
    "        for motifs_idx, nn_idx in zip(final_motifs.get(k), final_nn.get(k)):\n",
    "            \n",
    "            axs[k].plot(df[dim_name].iloc[motifs_idx : motifs_idx + m], c='red', linewidth=4)\n",
    "            axs[k].plot(df[dim_name].iloc[nn_idx : nn_idx + m], c='red', linewidth=4)\n",
    "            axs[k].axvline(x=motifs_idx, linestyle=\"dashed\", c='black')\n",
    "            axs[k].axvline(x=nn_idx, linestyle=\"dashed\", c='black')\n",
    "            \n",
    "            axs[k + mps.shape[0]].plot(motifs_idx, mps[k, motifs_idx] + 1, marker=\"v\", markersize=10, color='red')\n",
    "            axs[k + mps.shape[0]].plot(nn_idx, mps[k, nn_idx] + 1, marker=\"v\", markersize=10, color='red')\n",
    "\n",
    "            axs[k + mps.shape[0]].axvline(x=motifs_idx, linestyle=\"dashed\", c='black')\n",
    "            axs[k + mps.shape[0]].axvline(x=nn_idx, linestyle=\"dashed\", c='black')\n",
    "            \n",
    "            axs[k + mps.shape[0]].text(motifs_idx, mps[k][motifs_idx], f\"{i+1}m\", fontsize=\"xx-large\")\n",
    "            axs[k + mps.shape[0]].text(nn_idx, mps[k][motifs_idx], f\"{i+1}n\", fontsize=\"xx-large\")\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "    axs[k + mps.shape[0]].set_ylabel(f\"P_{k}\", fontsize=10)\n",
    "    axs[k + mps.shape[0]].plot(mps[k], c='orange')\n",
    "    axs[k + mps.shape[0]].set_xlabel('Time', fontsize=10)    \n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
